---
title: "ToDo for Saagaar"
author: "Spiro Stilianoudakis"
date: "7/29/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# ToDo List

## Primary questions:

   + Can a simple feed forward neural network be optimally tuned to predict TAD boundary regions
      - How does a neural network compare to random forest (see random forest performance section for benchmark)
   + What are the optmal parameters of a deep neural network"
      - Number of hidden layers?
      - Number of nodes per layer?
      - Does regularization help?
      - Does including dropout help?
      - Batch size?
   + Try more complicated neural networks
      - Both RNNs and CNNs only work well with image data (or 2D data where columns have a sense of closeness about them; similar to image data). But you can read chapter 5 and chapter 6 to see if you think our problem can be fit into the necessary framework
      - LSTM (look at chapter 7 and 8 of Deep Learning with R)

## Secondary questions

   + How does a NNet with a distance type feature space compare to a feature space with overlap counts?
      - Is normalization necessary for an overlap count feature space?
   + How does the size of bins (resolution "r") impact performance (see section on obtaining data)
   + How does a model built simply on CTCF, RAD21, SMC3, and ZNF143 compare to a model built on all 26 transcription factor binding sites (TFBS)
   + Start by building a model on chromosomes 1-21 (omitting chr9) and test it on chr22. Then repeat this process for each holdout chr and calculate average performance

# Obtaining Data at other resolutions

## Example of how to obtain training and testing at 100kb resolution (100000 sized genomic bins) using a distance type feature space

```{r}
library(preciseTAD)

# First load in the list 26 of TFBS
# These will be used to constuct the feature space (i.e. the columns of the data matrix X1,...,X26)
data("tfbsList")
names(tfbsList)

# This is in a list of GRanges objects (read https://bioconductor.org/packages/release/bioc/vignettes/GenomicRanges/inst/doc/GenomicRangesIntroduction.html for more)
# It basically shows the genomic coordinates throughout the linear genome for each TFBS
length(tfbsList) #26 

# You can look at the first 6 coordinates for each of the 26 TFBS
#lapply(tfbsList, head)

# Loading in ARROWHEAD TAD coordinates 
domains <- read.table("./data/arrowhead_data/100kb/GM12878_domain_data_100000.b.txt", header=F)

# The first 3 columns are the start and end coordinates for each TAD identified by ARROWHEAD
# We need to extract the unique boundary coordinates to create the response vector Y
head(domains[,1:3])

# Extracting unique boundary coordinates
bounds.GR <- preciseTAD::extractBoundaries(domains.mat=domains, 
                                     preprocess=FALSE, 
                                     CHR=paste0("CHR", c(1:8,10:22)), 
                                     resolution=100000)

# View unique boundaries
bounds.GR

# Creating the training (built on CHR1-CHR21) and testing data (built on CHR22)
set.seed(123)
tadData_distance <- preciseTAD::createTADdata(bounds.GR=bounds.GR, #the unique boundaries
                                  resolution=100000, #size of genomic bins, should match the TAD data imported from ARROWHEAD
                                  genomicElements.GR=tfbsList, #this list of TFBS
                                  featureType="distance", #type of feature space
                                  resampling="rus", #type of resampling to balance the classes (here, random under sampling)
                                  trainCHR=paste0("CHR",c(1:8,10:21)), #the chromosomes to train on
                                  predictCHR="CHR22") #the chromosome to test on

# This creates a list of two objects: the training data and the testing data
# Notice the training data is balanced but the testing data is not, this is to preserve the unbiased nature of a separate holdout test set that is commonplace in machine learning/deep learning
dim(tadData_distance[[1]])
table(tadData_distance[[1]]$y)

dim(tadData_distance[[2]])
table(tadData_distance[[2]]$y)
```

Note: To obtain the training/testing data at different resolutions, simply run the above code but be sure to read in the right resolution of ARROWHEAD data (in the ./data/arrowhead/"X"kb folder) and adjust the resolution parameters in the extractBoundaries and createTADdata functions.

# Random Forest Performance

```{r}
# Run RF using preciseTAD package
# See https://www.edureka.co/blog/random-forest-classifier/ for comprehensive guide to random forests
set.seed(123)
tadModel <- TADrandomForest(trainData = tadData_distance[[1]], #training data 
                            testData = tadData_distance[[2]], #testing data
                            tuneParams = list(mtry = c(2,5,8,10,13,16,18,21,24,26), #number of different features to choose at each split
                                              ntree = 500, #number of trees
                                              nodesize = 1), #minimum node size
                            cvFolds = 3, #number of cross validation folds to tune mtry parameter
                            cvMetric = "Accuracy", #metric to use for tuning
                            verbose = TRUE, #prints details if TRUE
                            model = TRUE, #keeps model object if TRUE
                            importances = TRUE, #keeps variable importances if TRUE
                            impMeasure = "MDA", #type of importance measure for variable importances (mean decrease in accuracy)
                            performances = TRUE) #keeps performances if TRUE

# View model performance
tadModel[[3]]
```

So the accuracy of a RF model built on 100kb genomic bins with a distance type feature space is only 0.587. Try it on higher resolutions (i.e. 5kb or 10kb) and see what the performances are. They will give you an idea of what to compare your neural network with.


# Resources

   + Installing keras/tenserflow in R: http://rstudio-pubs-static.s3.amazonaws.com/415380_56d75ae905a7418ca07f0040e0cbd70e.html
   + Dr. Dozmorov's neural network workshop page: https://bios691-deep-learning-r.netlify.app/
   + Github repo with code from textbook: https://github.com/jjallaire/deep-learning-with-r-notebooks
   + Video by textbook author on functionality of Kera in R: https://www.youtube.com/watch?v=atiYXm7JZv0
   + Great website with videos and tutorials (in python unfortunately): https://niessner.github.io/I2DL/
   + Discussion on our to optimize a nnet: https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw#:~:text=The%20number%20of%20hidden%20neurons,size%20of%20the%20input%20layer.
   
   
If you have any questions email me at stilianoudasc@mymail.vcu.edu